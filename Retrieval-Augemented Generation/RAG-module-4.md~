# RAG Module 4: LLMs and Text Generation

## Learning Objectives

- Explain the components of a transformer and how they impact the operation of an LLM inside 
a RAG system  
- Choose the appropriate LLM for a project based on qualities like cost, context window, knowledge cutoff, and performance on LLM benchmarks  
- Explain the operation of techniques including LLM sampling strategies, prompt engineering, and hallucination detection, and evaluate how best to use these techniques inside a RAG system  
- Identify the benefits of agentic workflows or fine-tuned LLMs and assess when they could be used to improve the performance of a RAG system  
- Build a RAG system that uses a variety of advanced techniques to improve the quality of an LLM's performance  

## Introduction

## Transformer Architecture

- Why RAG Works  
  - Attention mechanism processing  
  - World knowledge in feed forward layers  
  
- Inherent randomness remains  
  - LLMs may rtandoml;y inject infoirmation  
  - Need to control randomnes  
  - Must confirm LLMs ground info in retrieved information  
    
- Comjputational Expenses  
  - Generating single tokens requires extensive processing  
  - Costs grow with prompt/completion graph  
  - Each token must examine all other for content  
  - Most RAG system costs cxome from running transformers   
  
## LLM Sampling Strategies

- Every token generated by a weighted random choice  
- Peaked distribution model is "confident"  
- Flat distribution model is "uncertain"  
  
Decoding Strategies  
- **Greedy decoding**  
  - Always pick token with highest probability  
  - Determi9nistic  
  - Generic-sounding  
  - Can get stuck in a loop  
  - Useful in code completion or debugging  
- **Temperature**  
  - Changes the shpae of the distribution of tokens  
  - Temperature of 0 ==> Greedy decoding  
    - Deterministic  
    - One token has 100% probability  
 - Same output given same prompt  
  - Temperature of 5 ==> Flat distribution   
    - Generates nonsense  
    - All tokens have equal probability  
 - **Advanced Token Sampling**  
   - Top-K Sampling  
     - Picks from the top K most likely tokens  
	 - Can be combined with temperature  
   - Top-P Sampling  
     - Pick from tokens whose cummalative probability is below some threshold  
 - **Token Specific Strategies**  
     - Repetition penalties  
	 - Helps prevent: Loops, Redundant phrases, Overuse of specific words  
	 - Logit biases  
	   - Direct manipulation of token probabilities  
	   - Can add or subtract from the values  
	     - Profanity: Severely lower probability of those words  
	     - Classifier: Significantly boost probability of each class  
    
  
## Exploring LLM Capabilities (LAB)

- temperature
- top-K
- top-P

## Choosing Your LLM

LLM Characteristics
- Model Size
  - Small models: 1-100 billion parameters
  - Largel models: 100-500 billion parameters
  - Large models can be more capable, always more expesnsive
- Cost
  - Fixed cost per millions of tokens
  - New and larger moodels usually cost more
  - Different pricing for inpput vs output tokens
- Context window
  - Maximmum number of tokens split between prompt & output
  - Still pay per token
- Latency and speed
  - Time to first token
  - Tokens per second
- Training cutroff date
  - Last point in time of models training data
  - Later is usually preferable

LLM Quality Metrics
  - Much harder to quantify
  - Many types of quality
  - Numerous benchmarls that attempt to measure qualioty
  - No single authorotative list of benchmarks
  
Three types of benmarks
   - Automated
   - Human scoring
   - LLM as a judge
   
## Promp[t Engineering: Building Your Augmented Prompt

Messages format
- Role
  - System
  - User
  - Assistant

Message turns
  - Previous messages are not remembered
  - Passed in the next prompt
  
System Prompt
  - Provides high lervel instructions
  - Includes desired tone
  - Includes procedures LLM should follow
  
Principles for constructring system prompt
  - Instruct about response
    - E.g "Povide great detail"
    - Or "Answer q
  

  

 

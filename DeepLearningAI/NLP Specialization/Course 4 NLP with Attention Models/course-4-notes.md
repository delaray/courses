# Natural Language Processing with Attention Models  

In Course 4 of the Natural Language Processing Specialization, you will:  

a) Translate complete English sentences into German using an encoder-decoder attention model.  
b) Build a Transformer model to summarize text.  
c) Use T5 and BERT models to perform question-answering.  
d) Build a chatbot using a Reformer model.  
  
  
----

## Week 1: Neural Machine Translation with Attention models  

Discover some of the shortcomings of a traditional seq2seq model and how to solve for them by adding an attention mechanism, then build a Neural Machine Translation model with Attention that translates English sentences into German.  
  
  
----
  
## Week 2: Text Summarization with Transformer models  

Compare RNNs and other sequential models to the more modern Transformer architecture, then create a tool that generates text summaries.  
  
  
----

## Week 3: Question-Answering  

Explore transfer learning with state-of-the-art models like T5 and BERT, then build a model that can answer questions.  
  
  
----
  
  
## Week 4: Chatbots with Reformer models  

Examine some unique challenges Transformer models face and their solutions, then build a chatbot using a Reformer model.  
  
  
----


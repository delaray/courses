#  Natural Language Processing with Probabilistic Models

In Course 2 of the Natural Language Processing Specialization, you will:

a) Create a simple auto-correct algorithm using minimum edit distance and dynamic programming.  
b) Apply the Viterbi Algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics.  
c) Write a better auto-complete algorithm using an N-gram language model.  
d) Write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.  
  
  
----

## Week 1: Auto-correct

Learn about autocorrect, minimum edit distance, and dynamic programming, then build your own spellchecker to correct misspelled words.  

----

## Week 2: Part-of-Speech (POS) Tagging and Hidden Markov Models

Learn about Markov chains and Hidden Markov models, then use them to create part-of-speech tags for a Wall Street Journal text corpus.
  
  
----
  
## Week 3: Auto-complete and Language Models  
  
Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter.  
  
  
----

## Week 4: Word Embeddings with Neural Networks  

Learn how word embeddings carry the semantic meaning of words, making them more powerful for NLP tasks. Then build your own Continuous bag-of-words model to create word embeddings from Shakespeare text.  



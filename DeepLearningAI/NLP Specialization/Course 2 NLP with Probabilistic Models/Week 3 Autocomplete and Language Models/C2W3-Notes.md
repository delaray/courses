# C2W3: Autocomplete 


Video: Week Introduction (1 min)

----

## Video: N-Grams: Overview (3 min)

- Create an N-Gram model from a corpus
- Language models compute probs of sentences
- Estiamte prob of word sequences
- Estimate prob word follows a seq

Other appklications
- Speech recog
- Spelling correction
- Augmentive communication

## Reading: N-Grams Overview (5 min)

- Objectives
  - Process text corpus to N-Gram
  - Out of vocan words
  - Smoothing for unseen N-Grams
  - Language model evaluation


## Video: N-grams and Probabilities (7 min)

- N-gram is sequence of words (or other things)
- Unigram: All sets of single woirds
- Bigram: All seq of 2  words
- Trigrams: All seq of 3 words

- P(Unigram) = Word count / corpous size

- P(Bigram)
  - P(x|y) = P(y|x) / sum(w)C(x|w) = C(x|y) / C(x)
  
  
## Reading: N-grams and Probabilities (10 min)

- Compute approxmate seq prob instead because low likelihood
- Approximate: Product of the joint bigram probabilities

## Video: Sequence Probabilities (5 min)

- Sequence Prob.
- Approximate Seq Prob
  - Used the Markov assumption
  
Reading: Sequence Probabilities (6 min)

----

## Video: Starting and Ending Sentences (8 min)

- If working with n-token sliding window

- Start sentence
  - How to deal wioth start & end
  - Add a start token <s> for bigrams
  - Add 2 start token <s> <s> for trigrams
  - Generalize to N tokens

- End sentence
  - Issue when counting bigrams where last word is previous word
  - Add </s> special token
  - Only need to add one </s> for N-Grams
  
## Reading: Starting and Ending Sentences (6 min)

We usually start and end a sentence with the following tokens respectively: <s> </s>. 

When computing probabilities using a unigram, you can append an <s> in the beginning of the sentence.
To generalize to an N-gram language model, you can add N-1 start tokens <s>. 

For the end of sentence token </s>, you only need one even if it is an N-gram.

## Lab: Lecture notebook: Corpus preprocessing for N-grams (1h)

## Video: The N-gram Language Model (6 min)

- Count Matrix
  - Count of number sof relative n-grams
  
- Probability matrix
  - Transform count matrix into probability matrix
  
- Count Matrix:
  Rows: Unique corpuys of (N-1)-grams
  Columns: unique corpus words
  N-gram: row (n-1)-gram X column unigram
  Cell contains count of the N-gram
  
- Probability Matrix:
  - Divide each cell of the Count Matrix by sum of that row
  
- Language Model
  - Map probaility matrix to a language model
  - Language model assigns a probability to each sentence in corpus
  - Sentence probability = product joint probabilities
  - Use log probabilities because of
    - product of small numbers
	- avoid underflow
	
- Generative Language Model
  
## Reading: The N-gram Language Model (10 min)

We covered a lot of concepts in the previous video.  You have seen: 

- Count matrix
- Probability matrix
- Language model
- Log probability to avoid underflow
- Generative language model

In the count matrix:

- Rows correspond to the unique corpus N-1 grams. 

- Columns correspond to the unique corpus words. 


## Video: Language Model Evaluation (6 min)

- Measurement: Perpexlity metric
  - Divide into: Train, Validate, Test
  - Think of perplexity asa measure of complexity
  - Use perplexity to distiguish human from non-human
  - Perplexity is closaely related to entropy (measures uncertainty)
  - Small perplexity = better model
  - Some people use log(PP(W)) = -1/m * sum(m)(log2)=(P(w(i)|w(i-1)))
  - Good model has: 
	- Perplexity from 20 - 60 or,
	- Log Perplexity from 4.3 - 5.9
	

## Lab: Lecture notebook: Building the language model (1h)

- Pandas: **df[col][row]** or **df.loc[row][col]**

- The type of ('vocabnulary') is **str**
- The type of ('vocabnulary',) is **tuple**
  

## Reading: Language Model Evaluation (10 min)

Perplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.  
  

## Video: Out of Vocabulary Words (4 min)

- Closed vs Open vocabularies
- Model unknown woirds with <UNK>
  - Create vocabulary 
  - Replace words in corpus and bnot in V by <UNK>
  - Count bthe probabilities with <UNK> as any other word
  - Replace words with frequewncy < 2 with <UNK> as well
- Other techniques exist

- How to create vocabulary V
  - Min word frequency f
  - max |V|, include words by freq.
  
  - Use <UNK> spartingly
  
  - Perplexity: Only compare LMs with the same V
  



## Reading: Out of Vocabulary Words (10 min)

## Video: Smoothing (6 min)

- 
  - Missing N-grams in corpus
  - Smoothing
  - Backoff
  - Interpolation
  
- Problem: N-grams made of known words but not in training corpus

- Add-one smoothing (Laplacian smoothing
  - Add 1 in the nominator
  - Add |V| to denominator


- Add-k smoothing (larger corporar)
  - Add k in the nominator
  - Add |V|*k to denominator
  
- Backoff
  - If too many N-grams missing
  - Use (N-1)-grams or (N-2)-grams, etc..
  - Distorts probabilities
    - Use probability dfiscountring
	- Aka Katz Backoff
	
- Linear Interpolation

  

## Reading: Smoothing (10 min)

## Lab: Lecture notebook: Language model generalization (1h)

## Video: Week Summary (1 min)


## Reading: Week Summary (2 min)

This week you learned the following concepts
  - N-Grams and probabilities
  - Approximate sentence probability from N-Grams
  - Build a language model from a corpus
  - Fix missing information
  - Out of vocabulary words with <UNK>
  - Missing N-Gram in corpus with smoothing, backoff and interpolation
  - Evaluate language model with perplexity
  - Coding assignment! 

## Video: Week Conclusion (46 sec)

- Assignment:
  - Build LM from Corpus using N-Grams
  - Evaluate LM with Prplerxity Metric
  - Build Auto-correct system

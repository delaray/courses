# C2W2: Part of Speech Tagging

## Video: Week Introduction (1 min)

- Part of Speech (POS) Tagging
- Hidden Markov Models
- Viterbri Algorithm

## Video: Part of Speech Tagging (2 min)

- What is POS
  - Use shorthand tags
  
- Application of POS
  - Named entities
  - Co-reference resolutin
  - Speech Recognition
  
- Markov Chains
- Hidden Markov Models
- Viterbi Algorithm
- Example

## Reading: Part of Speech Tagging (4 min)

Part of Speech Tagging (POS) is the process of assigning a part of speech to a word.__
By doing so, you will learn the following:__

- Markov Chains
- Hidden Markov Models
- Viterbi algorithm

You can use part of speech tagging for:__

- Identifying named entities__
- Speech recognition__ 
- Coreference Resolution__

## Lab: Lecture Notebook - Working with text files (20mn)

- Python string module
  - string.punctuation

## Video: Markov Chains (3 min)

- Type of stochastyic mode that describes sequence of events
- Needs only probabiulity of previous event
- Models components that havre a random aspect to them


## Reading: Markov Chains (3 min)

- You can use Markov chains to identify the probability of the next word.
- You can see that the most likely word after a verb is a noun. 

## Video: Markov Chains and POS Tags (4 min)

- POS are states
- Transition probabilities
  - Probability of going from one state to another
  
- Markov Property
  - You only need the current state to determine next state
  
- Transition Matrix
  - Conisys all states by all states
  - A Table of all possible transition probabilities
  - Rows and columns should sum to 1
  
- Issue with probability first word
  - Add an initial state row for first word
  - Transition Matrix an actual matrix
    Dimensions: (N+1, N)

## Reading: Markov Chains and POS Tags (6 min)

- To help identify the parts of speech for every word, you need to build a transition matrix that gives you the probabilities from one state to another.  

- In the diagram above, the blue circles correspond to the part of speech tags, and the arrows correspond to the transition probabilities from one part of speech to another. You can populate the table on the right from the diagram on the left. The first row in your A matrix corresponds to the initial distribution among all the states. 


## Video: Hidden Markov Models (3 min)

## Reading: Hidden Markov Models (6 min)

## Video: Calculating Probabilities (3 min)

## Reading: Calculating Probabilities (5 min)

## Video: Populating the Transition Matrix (4 min)

## Reading: Populating the Transition Matrix (6 min)

## Video: Populating the Emission Matrix (2 min)

## Reading: Populating the Emission Matrix (5 min)

Lab: Lecture Notebook - Working with tags and Numpy
. Duration: 20 minutes20 min

## Video: The Viterbi Algorithm (4 min)

## Reading: The Viterbi Algorithm (5 min)

## Video: Viterbi: Initialization (2 min)

## Reading: Viterbi Initialization (5 min)

## Video: Viterbi: Forward Pass (2 min)

## Reading: Viterbi: Forward Pass (10 min)

## Video: Viterbi: Backward Pass (5 min)

## Reading: Viterbi: Backward Pass (10 min)

## Video: Week Conclusion (1 min)

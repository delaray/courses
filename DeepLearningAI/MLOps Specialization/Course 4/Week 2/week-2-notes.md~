# Week 2: Model Serving Patterns and Infrastructure  

Learn how to serve models and deliver batch and real-time inference results by building scalable and reliable infrastructure  

## Learning Objectives

- Serve models and deliver inference results by building scalable and reliable infrastructure.
- Contrast the use case for batch and realtime inference and how to optimize performance and hardware usage in each case
- Implement techniques to run inference on both edge devices and applications running in a web browser
- Outline and structure your data preprocessing pipeline to match your inference requirements
- Distinguish the performance and resource requirements for static and stream based batch inference

# How Diffusion Models Work  

## Introduction  

## Intuition  

The neural network learns to take noisy images and turn them back into sprites.
First we train a NN to predict noise from an image.

## Sampling  

DDPM: Denoising Diffusion Probabilistic Models

We train a Neural network to predict noise. We then use the NN to predict noise at each step and we subtract that noise using the DDPM algorithm from the input image to produce the next image in the iterative process.  

Because the NN expects a noisy image as input we must also add some additional noise scaled by timestamp back in before the next iteration otherwise the iterative process converges to some average blob of an image.  

    extra_noise = random sample if t>1 else 0
    predicted_noise = trained_nn(x(t-1), t)
    s1, s2, s3 = ddpm_scaling(t)
    sample = s1 * (sample - s2 * predicted_noise) + s3 * extra_noise


## Neural Network  

Uses a *UNet architecture*.  
Takes an image and produces noise image with same size.  
UNet dates back to 2015.  
Downsamples image several times using CNN into compressed representation.  
Then it upsamples same number of times into the output image.  
UNet can take in additional information in the form of embeddings  
  Add  time step embedding to the upsampling blocks.  
  Multiply context embedding info
    e.g. a text description pertaining to desired output
  
## Training  

Train a NN to learn to predict noise
Really learns the distribution of what is not noise

Take an image and a noise image and add them together.
Use the noise image as the label in order to define loss function

For a stable training scheme the noise image is generated by randomly sampling a time step.
Also Use different images in a single epoch

Algorithm
- Sample training image
- Sample timestep t (noise level)
- Sample noise
- Add noise to image
- Input image into NN and predict noise
- Compute loss between predicted and actual
- Backprop

## Controlling  

In order to control what the model creates we use vector embeddings of context.
Both the context embedding and the image+noise are fed into the NN during training
E.g, Image of Avocado + Noise and context "A ripe avocado".
E.g. Image of armchair + Noise and context "A comfy armchair"

Can then predict things the model has never seen:
E.g. Using the context "Avocado armchair"

Context is a vector for controlling generation
Context can be text embeddings 
Context can be categories as 1-hot encoded vectors

## Speeding Up  

Use a sampling stategy that is 10x more efficient than DDPM

DDIM: Denoising Diffusion Implicit Models

Faster because it skips timesteps
Specify a step-size as input
Tradeoff with quality

## Summary  

Stable Diffusion 
- Uses a method called latent diffusion
- Uses image embeddings rather than images
Cross-Attention 
Text Conditioning
Classifier-Free Guidance

